{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \\\n",
    "  langchain \\\n",
    "  langchain-community \\\n",
    "  langchain-core \\\n",
    "  langchain-google-genai \\\n",
    "  langchain-pinecone \\\n",
    "  pinecone-client \\\n",
    "  google-search-results \\\n",
    "  wikipedia \\\n",
    "  sentence-transformers \\\n",
    "  python-dotenv \\\n",
    "  tqdm \\\n",
    "  requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/agent_bot/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:367: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you have two containers for storing items:\n",
      "\n",
      "*   **List:** Like a shopping list you can change. You can add items, remove items, or change the order of items.  It's **mutable** (changeable).  You write it using square brackets: `[apple, banana, cherry]`\n",
      "\n",
      "*   **Tuple:** Like a set of coordinates (x, y). Once you define them, you can't change them. You can't add, remove, or change the order. It's **immutable** (unchangeable). You write it using parentheses: `(10, 20)`\n",
      "\n",
      "**In short:**\n",
      "\n",
      "*   **List:** Changeable, like a shopping list.\n",
      "*   **Tuple:** Unchangeable, like coordinates.\n",
      "\n",
      "**Why use tuples?**\n",
      "\n",
      "*   **Safety:**  Guarantees data won't be accidentally modified.\n",
      "*   **Speed:**  Tuples are generally a bit faster than lists because Python knows they won't change.\n",
      "*   **Keys in Dictionaries:**  Tuples can be used as keys in dictionaries, while lists cannot (because dictionary keys must be immutable).\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Initialize Gemini Flash LLM\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",  # This internally uses Flash for fast responses\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    convert_system_message_to_human=True,\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "# Test it\n",
    "response = llm.invoke(\"Explain the difference between Python lists and tuples in simple terms.\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_experimental\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Wikipedia tool\n",
    "wiki = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "# Initialize the Python REPL tool\n",
    "python_repl = PythonREPLTool()\n",
    "\n",
    "# Initialize the SERP API tool\n",
    "search = SerpAPIWrapper(serpapi_api_key=os.getenv(\"SERPAPI_API_KEY\"))\n",
    "\n",
    "# Bundle the tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Python_REPL\",\n",
    "        func=python_repl.run,\n",
    "        description=\"Useful for executing Python code for tasks such as math, logic, and plotting.\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Wikipedia\",\n",
    "        func=wiki.run,\n",
    "        description=\"Useful for retrieving factual and historical information from Wikipedia.\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Web_Search\",\n",
    "        func=search.run,\n",
    "        description=\"Useful for answering current event or factual queries using the SERP API.\",\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/agent_bot/lib/python3.10/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import uuid\n",
    "import os\n",
    "\n",
    "def create_pinecone_retriever_from_pdf(pdf_path: str, index_name=\"doc-chatbot\") -> tuple:\n",
    "    \"\"\"\n",
    "    Ingest a PDF file, upload to Pinecone under a unique namespace, and return a retriever + namespace.\n",
    "    \"\"\"\n",
    "    # Init Pinecone\n",
    "    pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "    # Load and split PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "\n",
    "    # Embedding model\n",
    "    embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Unique namespace\n",
    "    namespace = f\"session-{uuid.uuid4()}\"\n",
    "\n",
    "    # Upload to Pinecone\n",
    "    vectorstore = PineconeVectorStore.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embed_model,\n",
    "        index_name=index_name,\n",
    "        namespace=namespace\n",
    "    )\n",
    "\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    return retriever, namespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3r/z4x86ysd16l05yl4nzy8wrg80000gn/T/ipykernel_75733/3078288708.py:24: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"/Users/praddy/Documents/CODE/learning_langchain_folder/chatbot-docqa/data/attention.pdf\"  # You’ll dynamically pass this later via Streamlit\n",
    "retriever, namespace = create_pinecone_retriever_from_pdf(pdf_path)\n",
    "\n",
    "# Wrap as LangChain tool\n",
    "rag_tool = Tool(\n",
    "    name=\"Document_Retrieval\",\n",
    "    func=lambda q: retriever.get_relevant_documents(q),\n",
    "    description=(\n",
    "        \"Use this tool to answer any questions related to the user's uploaded PDF document. \"\n",
    "        \"It contains detailed information the user may reference or ask questions about.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "tools.append(rag_tool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3r/z4x86ysd16l05yl4nzy8wrg80000gn/T/ipykernel_75733/4247999525.py:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n",
      "/opt/anaconda3/envs/agent_bot/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:367: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documents, the main idea discussed is the **attention mechanism**, specifically within the context of the Transformer model architecture. The documents cover:\n",
      "\n",
      "*   The Transformer model architecture using stacked self-attention.\n",
      "*   The encoder and decoder stacks within the Transformer, including multi-head self-attention.\n",
      "*   Visualizations of the attention mechanism, showing how it follows long-distance dependencies and is involved in anaphora resolution.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the main idea discussed in the uploaded document?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Then send that as context to the LLM\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "response = llm.invoke([\n",
    "    HumanMessage(content=f\"Based on the following context, answer the question:\\n\\n{docs}\\n\\nQuestion: {query}\")\n",
    "])\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "# Initialize the agent\n",
    "agent = initialize_agent(\n",
    "    tools=tools,                     # All your tools, including RAG\n",
    "    llm=llm,                         # Gemini Flash\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # Most compatible agent type\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/agent_bot/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:367: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI should start by using Wikipedia to get a general overview of the attention mechanism in LLMs.\n",
      "Action: Wikipedia\n",
      "Action Input: attention mechanism in large language models\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPage: Large language model\n",
      "Summary: A large language model (LLM) is a type of machine learning model designed for natural language processing tasks such as language generation. LLMs are language models with many parameters, and are trained with self-supervised learning on a vast amount of text.\n",
      "The largest and most capable LLMs are generative pretrained transformers (GPTs). Modern models can be fine-tuned for specific tasks or guided by prompt engineering. These models acquire predictive power regarding syntax, semantics, and ontologies inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.\n",
      "\n",
      "Page: Transformer (deep learning architecture)\n",
      "Summary: The transformer is a deep learning architecture that was developed by researchers at Google and is based on the multi-head attention mechanism, which was proposed in the 2017 paper \"Attention Is All You Need\". Text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.\n",
      "Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM). Later variations have been widely adopted for training large language models (LLM) on large (language) datasets.\n",
      "\n",
      "Transformers were first developed as an improvement over previous architectures for machine translation, but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, audio, multimodal learning, robotics, and even playing chess. It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs) and BERT (bidirectional encoder representations from transformers).\n",
      "\n",
      "Page: Attention Is All You Need\n",
      "Summary: \"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wide variety of AI, such as large language models. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique's potential for other tasks like question answering and what is now known as multimodal Generative AI.\n",
      "The paper's title is a reference to the song \"All You Need Is Love\" by the Beatles. The name \"Transformer\" was picked because Jakob Uszkoreit, one of the paper's authors, liked the sound of that word.\n",
      "An early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers animated show. The team was named Team Transformer.\n",
      "Some early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.\n",
      "As of 2025, the paper has been cited more than 173,000 times.\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/agent_bot/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:367: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThe attention mechanism is a key component of the transformer architecture, which is widely used in large language models (LLMs). The transformer, introduced in the \"Attention Is All You Need\" paper, relies on multi-head attention to contextualize tokens within a context window, amplifying important tokens and diminishing less important ones. This architecture has advantages over recurrent neural networks (RNNs) like LSTMs, requiring less training time. The transformer architecture has become foundational in modern AI and has been applied to various tasks beyond machine translation, including natural language processing, computer vision, and more.\n",
      "Final Answer: The attention mechanism is a key component of the transformer architecture, which is widely used in large language models (LLMs). The transformer, introduced in the \"Attention Is All You Need\" paper, relies on multi-head attention to contextualize tokens within a context window, amplifying important tokens and diminishing less important ones. This architecture has advantages over recurrent neural networks (RNNs) like LSTMs, requiring less training time. The transformer architecture has become foundational in modern AI and has been applied to various tasks beyond machine translation, including natural language processing, computer vision, and more.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Bot: The attention mechanism is a key component of the transformer architecture, which is widely used in large language models (LLMs). The transformer, introduced in the \"Attention Is All You Need\" paper, relies on multi-head attention to contextualize tokens within a context window, amplifying important tokens and diminishing less important ones. This architecture has advantages over recurrent neural networks (RNNs) like LSTMs, requiring less training time. The transformer architecture has become foundational in modern AI and has been applied to various tasks beyond machine translation, including natural language processing, computer vision, and more.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/agent_bot/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:367: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI need to use the Document_Retrieval tool to find information about the mechanism of attention from the uploaded document.\n",
      "Action: Document_Retrieval\n",
      "Action Input: mechanism of attention\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[Document(id='35fff3ea-b2b9-4ef6-a6d1-20e4633301d2', metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 12.0, 'page_label': '13', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/praddy/Documents/CODE/learning_langchain_folder/chatbot-docqa/data/attention.pdf', 'subject': '', 'title': '', 'total_pages': 15.0, 'trapped': '/False'}, page_content='the\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'), Document(id='778e0b80-66b2-45d9-a2ce-f0959f445f5a', metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 4.0, 'page_label': '5', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/praddy/Documents/CODE/learning_langchain_folder/chatbot-docqa/data/attention.pdf', 'subject': '', 'title': '', 'total_pages': 15.0, 'trapped': '/False'}, page_content='3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].'), Document(id='269e6c98-7032-4244-a836-4921e3aaa91a', metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 13.0, 'page_label': '14', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/praddy/Documents/CODE/learning_langchain_folder/chatbot-docqa/data/attention.pdf', 'subject': '', 'title': '', 'total_pages': 15.0, 'trapped': '/False'}, page_content='.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'), Document(id='e80973c1-4545-4c9c-bdfb-33be741c8bc8', metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 6.0, 'page_label': '7', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/praddy/Documents/CODE/learning_langchain_folder/chatbot-docqa/data/attention.pdf', 'subject': '', 'title': '', 'total_pages': 15.0, 'trapped': '/False'}, page_content='the approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching')]\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/agent_bot/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:367: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThe document provides insights into the attention mechanism, particularly within the context of the Transformer model. Here's a breakdown:\n",
      "\n",
      "*   **Encoder-Decoder Attention:** The Transformer uses multi-head attention in \"encoder-decoder attention\" layers. In this setup, queries originate from the previous decoder layer, while memory keys and values come from the encoder's output. This allows each position in the decoder to attend to all positions in the input sequence, mimicking typical encoder-decoder attention mechanisms.\n",
      "\n",
      "*   **Self-Attention for Interpretability:** The document suggests that self-attention can lead to more interpretable models. By inspecting attention distributions, the authors found that individual attention heads learn to perform different tasks, and many exhibit behavior related to the syntactic and semantic structure of sentences.\n",
      "\n",
      "*   **Examples of Attention Heads:** The document includes examples of attention heads involved in tasks like following long-distance dependencies (e.g., \"making...more difficult\") and anaphora resolution (identifying what \"its\" refers to). These examples illustrate how attention mechanisms can capture relationships between words in a sentence.\n",
      "Action: Document_Retrieval\n",
      "Action Input: self-attention\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[Document(id='ff0348ff-a629-4ddf-9962-62667bd4d569', metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 1.0, 'page_label': '2', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/praddy/Documents/CODE/learning_langchain_folder/chatbot-docqa/data/attention.pdf', 'subject': '', 'title': '', 'total_pages': 15.0, 'trapped': '/False'}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].'), Document(id='e80973c1-4545-4c9c-bdfb-33be741c8bc8', metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 6.0, 'page_label': '7', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/praddy/Documents/CODE/learning_langchain_folder/chatbot-docqa/data/attention.pdf', 'subject': '', 'title': '', 'total_pages': 15.0, 'trapped': '/False'}, page_content='the approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching'), Document(id='b229130d-db5e-4441-81d7-e8e5dad4b2c1', metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 10.0, 'page_label': '11', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/praddy/Documents/CODE/learning_langchain_folder/chatbot-docqa/data/attention.pdf', 'subject': '', 'title': '', 'total_pages': 15.0, 'trapped': '/False'}, page_content='[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-'), Document(id='2d16a965-14d2-4451-a866-cb1c599b4b7e', metadata={'author': '', 'creationdate': '2024-04-10T21:11:43+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'page': 5.0, 'page_label': '6', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/Users/praddy/Documents/CODE/learning_langchain_folder/chatbot-docqa/data/attention.pdf', 'subject': '', 'title': '', 'total_pages': 15.0, 'trapped': '/False'}, page_content='P Epos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-')]\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/agent_bot/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:367: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mThe document describes the following about the mechanism of attention:\n",
      "\n",
      "*   **Self-Attention Definition:** Self-attention, also known as intra-attention, is an attention mechanism that relates different positions within a single sequence. This allows the model to compute a representation of the sequence by considering the relationships between its elements.\n",
      "*   **Applications of Self-Attention:** Self-attention has been successfully applied in various tasks, including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\n",
      "*   **Interpretability:** Self-attention can yield more interpretable models. Individual attention heads learn to perform different tasks, and many exhibit behavior related to the syntactic and semantic structure of sentences.\n",
      "*   **Encoder-Decoder Attention:** In \"encoder-decoder attention\" layers, queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence.\n",
      "*   **Examples:** The document provides examples of attention heads involved in tasks like following long-distance dependencies and anaphora resolution.\n",
      "Final Answer: The document describes the attention mechanism, particularly self-attention, as a way to relate different positions within a single sequence to compute a representation of that sequence. It highlights its applications in various NLP tasks and its potential for creating more interpretable models. The document also discusses encoder-decoder attention, where the decoder attends to the encoder's output. Examples are provided to illustrate how attention heads can capture relationships between words in a sentence, such as long-distance dependencies and anaphora resolution.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Bot: The document describes the attention mechanism, particularly self-attention, as a way to relate different positions within a single sequence to compute a representation of that sequence. It highlights its applications in various NLP tasks and its potential for creating more interpretable models. The document also discusses encoder-decoder attention, where the decoder attends to the encoder's output. Examples are provided to illustrate how attention heads can capture relationships between words in a sentence, such as long-distance dependencies and anaphora resolution.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/agent_bot/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:367: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI need to find out who captained the Indian cricket team in the most recent Champions Trophy tournament. I can use a web search to find this information.\n",
      "Action: Web_Search\n",
      "Action Input: \"who captained India in latest champions trophy\"\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mRohit Sharma\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/agent_bot/lib/python3.10/site-packages/langchain_google_genai/chat_models.py:367: UserWarning: Convert_system_message_to_human will be deprecated!\n",
      "  warnings.warn(\"Convert_system_message_to_human will be deprecated!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
      "\n",
      "Final Answer: Rohit Sharma\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Bot: Rohit Sharma\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    query = input(\"You: \")\n",
    "    if query.lower() in {\"exit\", \"quit\"}:\n",
    "        break\n",
    "    response = agent.invoke(query)\n",
    "    print(\"Bot:\", response[\"output\"] if isinstance(response, dict) else response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent_bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
